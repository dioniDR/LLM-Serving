# LLM-Serving

Una API ligera y eficiente para servir modelos de lenguaje de gran escala (LLMs) localmente.

## ğŸš€ DescripciÃ³n

LLM-Serving es una API REST minimalista diseÃ±ada para cargar y consultar modelos de lenguaje de manera sencilla en un entorno local. Este proyecto permite exponer diversos modelos en formato GGUF, ONNX y otros formatos comunes a travÃ©s de una interfaz HTTP unificada.

## âœ¨ CaracterÃ­sticas

- **Arquitectura ligera**: DiseÃ±ada para ser simple pero efectiva.
- **Multi-modelo**: Soporte para servir mÃºltiples modelos simultÃ¡neamente.
- **Flexibilidad de formatos**: Compatible con modelos en formato GGUF, ONNX y mÃ¡s.
- **Modo simulaciÃ³n**: Capacidad de probar la API sin modelos reales.
- **FÃ¡cil configuraciÃ³n**: GestiÃ³n de modelos a travÃ©s de un archivo JSON.

## ğŸ“‹ Requisitos

- Python 3.8+
- FastAPI
- Uvicorn
- Dependencias especÃ­ficas para los modelos (se detallan en `requirements.txt`)

## ğŸ”§ InstalaciÃ³n

### OpciÃ³n 1: Usando Docker (recomendado)

1. Clonar el repositorio:
```bash
git clone https://github.com/tu-usuario/llm-serving.git
cd llm-serving
```

2. Iniciar con Docker Compose:
```bash
docker-compose up -d
```

O usar el script incluido:
```bash
chmod +x docker-start.sh
./docker-start.sh
```

### OpciÃ³n 2: InstalaciÃ³n local

1. Clonar el repositorio:
```bash
git clone https://github.com/tu-usuario/llm-serving.git
cd llm-serving
```

2. Ejecutar el script de configuraciÃ³n:
```bash
chmod +x setup.sh
./setup.sh
```

3. Configurar modelos en `model_config.json`:
```json
{
  "mistral-7b": "models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
  "phi-2": "models/phi-2.onnx",
  "fake_model": "models/fake_model.txt"
}
```

## ğŸš€ Uso

### Iniciar el servidor

Con Docker:
```bash
docker-compose up -d
```

Sin Docker:
```bash
source venv/bin/activate  # Si usas entorno virtual
uvicorn main:app --reload
```

### Listar modelos disponibles

```bash
curl http://localhost:8001/models
```

### Consultar un modelo

```bash
curl "http://localhost:8001/query/mistral-7b?prompt=Explica la teorÃ­a de la relatividad"
```

### Modo de simulaciÃ³n

Para probar la API sin modelos reales:

1. Crear un archivo de prueba:
```bash
touch models/fake_model.txt
```

2. Agregar al archivo de configuraciÃ³n:
```json
{
  "fake_model": "models/fake_model.txt"
}
```

3. Consultar el modelo simulado:
```bash
curl "http://localhost:8001/query/fake_model?prompt=Hola"
```

## ğŸ› ï¸ Estructura del proyecto

```
llm-serving/
â”œâ”€â”€ main.py                # Punto de entrada de la aplicaciÃ³n
â”œâ”€â”€ routes.py              # DefiniciÃ³n de rutas API
â”œâ”€â”€ model_manager.py       # GestiÃ³n de modelos
â”œâ”€â”€ model_config.json      # ConfiguraciÃ³n de modelos
â”œâ”€â”€ models/                # Directorio para almacenar modelos
â”‚   â”œâ”€â”€ fake_model.txt     # Modelo de prueba
â”‚   â””â”€â”€ ...                # Otros modelos
â”œâ”€â”€ requirements.txt       # Dependencias
â”œâ”€â”€ Dockerfile             # ConfiguraciÃ³n para construir la imagen Docker
â”œâ”€â”€ docker-compose.yml     # ConfiguraciÃ³n para Docker Compose
â”œâ”€â”€ docker-start.sh        # Script para iniciar con Docker
â””â”€â”€ setup.sh               # Script para configuraciÃ³n local
```

## ğŸ“ Planes futuros

- Implementar streaming de respuestas
- AÃ±adir soporte para parÃ¡metros de generaciÃ³n avanzados
- Desarrollar una interfaz web simple
- Optimizar la inferencia para modelos grandes

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - ver el archivo LICENSE para mÃ¡s detalles.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, abre un issue para discutir los cambios importantes antes de enviar un pull request.
